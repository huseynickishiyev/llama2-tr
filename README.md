# Dolly Chatbot Fine-Tuning with databricks-dolly-15k-tr 

## Description

This project focuses on fine-tuning the NousResearch Llama 2.7B Chatbot model using techniques like gradient checkpointing, quantization (BitsAndBytes), and Parameter-Efficient Fine-Tuning (PEFT). The goal is to enhance the model's performance on a specific dataset ("atasoglu/databricks-dolly-15k-tr"). The dataset can be found on Hugging Face, but bear in mind that it needs cleaning for better results.

## Dependencies

- Python 3.6+
- PyTorch
- Hugging Face Transformers
- Datasets
- BitsAndBytes library

## Results

Currently, the project is in the initial stages of fine-tuning. Specific results and benchmarks will be updated as the model progresses.

## Acknowledgments

- NousResearch for providing the Llama 2.7B Chatbot model.
- Hugging Face for their Transformers library.
- Datasets community for the "atasoglu/databricks-dolly-15k-tr" dataset.
